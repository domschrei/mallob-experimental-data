
# logs/

Each directory corresponds to one certain run of Mallob. In each directory, you can find the following files and directories:

* `slurm-*.out`: The log file generated by SLURM, the job scheduler employed at the used supercomputer. These files also contain the configuration with which Mallob was launched for each run.
* `*/`: Each directory `i` contains all log files created by MPI rank `i`. Due to space limitations, we only provide these directories for rank zero as well as for each rank which we configured to introduce jobs to the system. Please let us know if you are interested in the full logs of an experiment.

Each directory contains the following files:

* `log.<rank>`: The output of the PE's main thread.
* `log.<rank>.janitor`: The output of the background thread responsible for cleaning up old job contexts.
* `log.<rank>.<balancing|treegrowth>-latencies`: Lists of raw floating-point values (in seconds) for individual event latencies.
* `subproc.<rank>`: The output of the PE's worker processes and threads.

In addition, each directory of a PE configured to introduce jobs contains the following files:

* `log.<rank>.i`, `log.<rank>.i-fs`, `log.<rank>.i-api`, `log.<rank>.streamer`: The output of background threads introducing jobs.
* `log.<rank>.reader`: The output of background threads parsing descriptions of introduced jobs.

## Extracting Data

General data can be extracted from the following log lines in particular:

* `<time> 0 sysstate busyratio=X cmtdratio=X jobs=X globmem=XGB newreqs=X hops=X`: This line is output every second by rank zero and gives useful information e.g. on the ratio of busy PEs, the number of active jobs, or the global RAM in use.
* `<time> <rank> sysstate entered=X parsed=X scheduled=X processed=X`: This line is output every second by the PE with the smallest rank among all PEs which introduce jobs to the system. We computed throughputs using the `processed` field.
* `<time> <rank> Scheduling r.#<jobid>:0 ... (latency: <latency>s)`: A new job for which an initial request message was introduced to the system can now be scheduled on a specific PE after `<latency>` seconds.
* `<time> <rank> EXECUTE #<jobid>:<treeindex> <= <sender>`: A new worker is initialized at a PE after receiving a job description from `<sender>`.
* `<time> <rank> LOAD <0|1> (<+|->#<jobid>:<treeindex>)`: A PE begins/continues ("+") or stops ("-") to execute a certain worker.
* `<time> <rank> #<jobid>:0 : update v=<volume> ...`: A job receives a volume update.

## Result Files

We provide some central data extracted from the log files:

* `mallob_priorities`: The file `priorities` contains the nine used priority values. The file `avg_volumes` contains for each job ID its average assigned volume over time. The jobs of highest priority have IDs < 100'000, the jobs of the second highest priority have IDs < 200'000, and so on. For each priority, the same set of 80 jobs has been processed repeatedly (to guarantee constant system utilization even if one of the streams already processed 80 jobs). Each file `response_times.<k>` contains successful response times among the first 80 jobs of k-th highest priority.
* `mallob_realistic_*`: Each run features files `init-latencies`, `balancing-latencies`, and `treegrowth-latencies` which contain the raw measured latencies (in seconds) for each such event (initial job scheduling, balancing result, growth of a job tree by another worker). Each file `job-context-overview` has four arguments per line: a job ID, its maximum assigned volume, the number of created workers, and the ratio between the second and third number. Each file `worker-creation-occurrences` contains a histogram on how often a worker was rescheduled (created at a PE other than the original PE). 
* More data will be made available later this week.
